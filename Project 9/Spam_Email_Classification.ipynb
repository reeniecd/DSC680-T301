{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e827c3e6",
   "metadata": {},
   "source": [
    "## Spam Email Classification: Building an Efficient Filter to Identify Unwanted Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35449cb0",
   "metadata": {},
   "source": [
    "Spam email classification is a crucial task in the realm of email communication. This project aims to develop a robust and accurate spam email classifier using machine learning techniques. By leveraging the \"SpamAssassin Public Corpus\" (https://spamassassin.apache.org/old/publiccorpus/) dataset, comprising a diverse collection of labeled spam and ham emails, this project will explore various text classification algorithms and feature extraction methods. The goal is to build a powerful spam email filter capable of accurately distinguishing between legitimate emails and unwanted spam messages. Through the implementation, training, and evaluation of different models, this project seeks to provide insights into effective techniques for email filtering, ultimately improving user experience and email security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452c532",
   "metadata": {},
   "source": [
    "### Data Sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f113c9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\chris\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f5d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the WordNet resource\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90d04ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the Open Multilingual WordNet resource\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c06959",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db51c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: new sequence window date wed 21 aug 2002 10 54 46 0500 chris garrigues message id 1029945287 4797 tmda deepeddy vircio com reproduce error repeatable like every time without fail debug log pick happening 18 19 03 pick_it exec pick inbox list lbrace lbrace subject ftp rbrace rbrace 4852 4852 sequence mercury 18 19 03 exec pick inbox list lbrace lbrace subject ftp rbrace rbrace 4852 4852 sequence mercury 18 19 04 ftoc_pickmsgs 1 hit 18 19 04 marking 1 hit 18 19 04 tkerror syntax error expression int note run pick command hand delta pick inbox list lbrace lbrace subject ftp rbrace rbrace 4852 4852 sequence mercury 1 hit 1 hit come obviously version nmh using delta pick version pick nmh 1 0 4 compiled fuchsia c mu oz au sun mar 17 14 55 56 ict 2002 relevant part mh_profile delta mhparam pick seq sel list since pick command work sequence actually one explicit command line search popup one come mh_profile get created kre p still using version code form day ago able reach cv repository today local routing issue think _______________________________________________ exmh worker mailing list exmh worker redhat com http listman redhat com mailman listinfo exmh worker\n",
      "Label: Ham\n",
      "\n",
      "Email: zzzzteana alexander martin posted tasso papadopoulos greek sculptor behind plan judged limestone mount kerdylio 70 mile east salonika far mount athos monastic community ideal patriotic sculpture well alexander granite feature 240 ft high 170 ft wide museum restored amphitheatre car park admiring crowd planned mountain limestone granite limestone weather pretty fast yahoo group sponsor 4 dvd free p join http u click yahoo com pt6ybb nxieaa mg3haa 7gsolb tm unsubscribe group send email forteana unsubscribe egroups com use yahoo group subject http doc yahoo com info term\n",
      "Label: Ham\n",
      "\n",
      "Email: zzzzteana moscow bomber man threatens explosion moscow thursday august 22 2002 1 40 pm moscow ap security officer thursday seized unidentified man said armed explosive threatened blow truck front russia federal security service headquarters moscow ntv television reported officer seized automatic rifle man carrying man got truck taken custody ntv said detail immediately available man demanded talk high government official interfax itar ta news agency said ekho moskvy radio reported wanted talk russian president vladimir putin police security force rushed security service building within block kremlin red square bolshoi ballet surrounded man claimed one half ton explosive news agency said negotiation continued one half hour outside building itar ta interfax reported citing witness man later drove away building police escort drove street near moscow olympic penta hotel authority held negotiation moscow police press service said move appeared attempt security service get secure location yahoo group sponsor 4 dvd free p join http u click yahoo com pt6ybb nxieaa mg3haa 7gsolb tm unsubscribe group send email forteana unsubscribe egroups com use yahoo group subject http doc yahoo com info term\n",
      "Label: Ham\n",
      "\n",
      "Email: irr klez virus die klez virus die already prolific virus ever klez continues wreak havoc andrew brandt september 2002 issue pc world magazine posted thursday august 01 2002 klez worm approaching seventh month wriggling across web making one persistent virus ever expert warn may harbinger new virus use combination pernicious approach go pc pc antivirus software maker symantec mcafee report 2000 new infection daily sign letup press time british security firm messagelabs estimate 1 every 300 e mail message hold variation klez virus say klez already surpassed last summer sircam prolific virus ever newer klez variant merely nuisance carry virus corrupt data http www pcworld com news article 0 aid 103259 00 asp _______________________________________________ irregular mailing list irregular tb tf http tb tf mailman listinfo irregular\n",
      "Label: Ham\n",
      "\n",
      "Email: insert signature wed aug 21 2002 15 46 ulises ponce wrote hi command insert signature using combination key sent mail insert simply put nmh component file component replcomps forwcomps way get editing message also using comp file specific folder alter sig per folder trick see doc n mh detail might must also way get sedit using gvim exmh message editor long time load command load email specific setting eg syntax colour highlight header quoted part email would possible map vim key would add sig even give selection sigs choose sort way randomly chosen sigs somewhere rtfm mit edu ok go rtfm mit edu pub usenet group news answer signature_finger_faq warning old may 1995 regard ulises hope help cheer tony _______________________________________________ exmh user mailing list exmh user redhat com http listman redhat com mailman listinfo exmh user\n",
      "Label: Ham\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import email\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define paths to the downloaded dataset folders\n",
    "ham_folder = 'C:\\\\Users\\\\chris\\\\DSC680-T301\\\\emailham'\n",
    "spam_folder = 'C:\\\\Users\\\\chris\\\\DSC680-T301\\\\emailspam'\n",
    "\n",
    "\n",
    "# Initialize lists to store preprocessed email data\n",
    "emails = []\n",
    "labels = []\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=' ')\n",
    "\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Return preprocessed text as a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Iterate through ham folder\n",
    "for filename in os.listdir(ham_folder):\n",
    "    with open(os.path.join(ham_folder, filename), 'r', encoding='latin1') as file:\n",
    "        # Parse email content\n",
    "        content = file.read()\n",
    "        msg = email.message_from_string(content)\n",
    "\n",
    "        # Extract subject and body\n",
    "        subject = msg.get('Subject', '')\n",
    "        body = ''\n",
    "        if msg.is_multipart():\n",
    "            for part in msg.walk():\n",
    "                content_type = part.get_content_type()\n",
    "                if content_type == 'text/plain':\n",
    "                    body = part.get_payload()\n",
    "        else:\n",
    "            body = msg.get_payload()\n",
    "\n",
    "        # Preprocess email text\n",
    "        preprocessed_text = preprocess_text(subject + ' ' + body)\n",
    "\n",
    "        # Append preprocessed email and label to lists\n",
    "        emails.append(preprocessed_text)\n",
    "        labels.append(0)  # 0 for ham\n",
    "\n",
    "# Iterate through spam folder\n",
    "for filename in os.listdir(spam_folder):\n",
    "    with open(os.path.join(spam_folder, filename), 'r', encoding='latin1') as file:\n",
    "        # Parse email content\n",
    "        content = file.read()\n",
    "        msg = email.message_from_string(content)\n",
    "\n",
    "        # Extract subject and body\n",
    "        subject = msg.get('Subject', '')\n",
    "        body = ''\n",
    "        if msg.is_multipart():\n",
    "            for part in msg.walk():\n",
    "                content_type = part.get_content_type()\n",
    "                if content_type == 'text/plain':\n",
    "                    body = part.get_payload()\n",
    "        else:\n",
    "            body = msg.get_payload()\n",
    "\n",
    "        # Preprocess email text\n",
    "        preprocessed_text = preprocess_text(subject + ' ' + body)\n",
    "\n",
    "        # Append preprocessed email and label to lists\n",
    "        emails.append(preprocessed_text)\n",
    "        labels.append(1)  # 1 for spam\n",
    "\n",
    "# Print a few preprocessed emails and their corresponding labels\n",
    "for i in range(5):\n",
    "    print(\"Email:\", emails[i])\n",
    "    print(\"Label:\", \"Spam\" if labels[i] == 1 else \"Ham\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0179846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (3052, 37356)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert the preprocessed emails to numerical features\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Print the shape of the feature matrix\n",
    "print(\"Shape of feature matrix:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb18b8c",
   "metadata": {},
   "source": [
    "There are 3052 preprocessed emails, and each email is represented by a vector of length 37356, where each element of the vector corresponds to a unique feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d528c63",
   "metadata": {},
   "source": [
    "### Train a machine learning model using the preprocessed emails and corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b59e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972176759410802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer and fit it on the training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621d8a3",
   "metadata": {},
   "source": [
    "SVM classifier achieved a high accuracy of 97.22% on the test data. This indicates that the model is performing well in distinguishing between spam and ham emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8955f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.8152173913043478\n",
      "F1 Score: 0.8982035928143712\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59bd153",
   "metadata": {},
   "source": [
    "Based on the evaluation metrics , the email classification model achieved the following results:\n",
    "\n",
    "A precision of 1.0 indicates that all the emails predicted as spam were indeed spam. The recall value of 0.8152173913043478 suggests that the model correctly identified approximately 81.52% of the actual spam emails. The F1 score of 0.8982035928143712 represents a balanced measure of precision and recall.\n",
    "\n",
    "Overall, these metrics indicate that the model is performing well in terms of precision, but there is some room for improvement in terms of recall. It's important to find the right balance between precision and recall based on your specific requirements and the costs associated with false positives and false negatives in email classification.\n",
    "\n",
    "If you have a large number of false negatives (spam emails classified as ham), you may want to focus on improving recall to catch more spam emails. On the other hand, if you have a high number of false positives (ham emails classified as spam), you may want to focus on improving precision to reduce the number of legitimate emails mistakenly classified as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6f6320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[519   0]\n",
      " [ 17  75]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752a7d4",
   "metadata": {},
   "source": [
    "1. True Positives (TP): 519 - The model correctly classified 519 emails as spam.\n",
    "2. True Negatives (TN): 75 - The model correctly classified 75 emails as ham.\n",
    "3. False Positives (FP): 0 - The model incorrectly classified 0 ham emails as spam.\n",
    "4. False Negatives (FN): 17 - The model incorrectly classified 17 spam emails as ham."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2063a88",
   "metadata": {},
   "source": [
    "## Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe6765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The email is predicted as ham.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=' ')\n",
    "\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Return preprocessed text as a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the email text\n",
    "email_content = \"\"\"\n",
    "Hi, \n",
    "\n",
    "Here are a few reasons why 2,300+ developers have signed up for ProjectPro :\n",
    "\n",
    "1) This is the only product in the world that provides pre-built, verified, end-to-end project recipes in Machine Learning and Big Data.\n",
    "\n",
    "2) Impress your boss by having on-demand access to pre-built, reusable project solutions using the latest frameworks like Tensorflow, PySpark, BERT etc. \n",
    "\n",
    "3) Get assigned to hot projects in Machine Learning and Big Data in your company and have the confidence to work on these projects with the help of our reusable solutions. \n",
    "\n",
    "4) Impress your job interviewers with implementation knowledge on a variety of real world live projects. \n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the email text\n",
    "preprocessed_email = preprocess_text(email_content)\n",
    "\n",
    "# Transform the preprocessed email text using the fitted vectorizer\n",
    "email_tfidf = vectorizer.transform([preprocessed_email])\n",
    "\n",
    "# Predict the label for the email\n",
    "email_prediction = svm_classifier.predict(email_tfidf)\n",
    "\n",
    "# Print the predicted label\n",
    "if email_prediction == 1:\n",
    "    print(\"The email is predicted as spam.\")\n",
    "else:\n",
    "    print(\"The email is predicted as ham.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547a26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
